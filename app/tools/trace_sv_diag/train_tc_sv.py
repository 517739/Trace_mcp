# train_tc_sv_new.py
# -*- coding: utf-8 -*-
"""
SV (Service-View) ä¼˜åŒ–ç‰ˆè®­ç»ƒè„šæœ¬
- å¼•å…¥ Context ç‰¹å¾ (å¦‚æœåœ¨ make é˜¶æ®µå·²æ„å»º)
- è¾“å‡º SVND é£æ ¼çš„è¯¦ç»†æŠ¥è¡¨ (æ··æ·†çŸ©é˜µã€F1ç­‰)
- ä½¿ç”¨å¤šä»»åŠ¡ Loss ç¨³å®šè®­ç»ƒ
"""

import os, json, argparse, torch
import torch.nn as nn
from tqdm import tqdm
from torch.utils.data import DataLoader

# å‡è®¾ model_sv å·²æŒ‰ä¸Šè¿°ç¬¬äºŒæ­¥ä¿®æ”¹
from model_sv import TraceClassifier 
# å‡è®¾ utils_sv å·²åŒ…å« dataset å®šä¹‰
from utils_sv import TraceDataset, collate, set_seed, vocab_sizes_from_meta, evaluate_detailed

# ================= ä¸»è®­ç»ƒé€»è¾‘ =================

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--data-root", default="dataset/tianchi/processed_sv_opt") # ä½ çš„æ–°æ•°æ®ç›®å½•
    parser.add_argument("--save-dir", default="logs/sv_optimized")
    parser.add_argument("--epochs", type=int, default=20)
    parser.add_argument("--batch", type=int, default=64)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--ctx-dim", type=int, default=3, help="Contextç»´åº¦ï¼Œå–å†³äºmakeè„šæœ¬")
    args = parser.parse_args()
    
    os.makedirs(args.save_dir, exist_ok=True)
    set_seed(2025)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # 1. åŠ è½½æ•°æ®
    print(f"ğŸ“– Loading data from {args.data_root}...")
    # fit_stats=True ä¼šè®¡ç®—å»¶è¿Ÿçš„å‡å€¼æ–¹å·®ç”¨äºå½’ä¸€åŒ–
    ds_tr = TraceDataset(os.path.join(args.data_root, "train.jsonl"), fit_stats=True)
    stats = ds_tr.stats
    ds_va = TraceDataset(os.path.join(args.data_root, "val.jsonl"), fit_stats=False, stats=stats)
    ds_te = TraceDataset(os.path.join(args.data_root, "test.jsonl"), fit_stats=False, stats=stats)

    tr_loader = DataLoader(ds_tr, batch_size=args.batch, shuffle=True, collate_fn=collate, num_workers=4)
    va_loader = DataLoader(ds_va, batch_size=args.batch, shuffle=False, collate_fn=collate)
    te_loader = DataLoader(ds_te, batch_size=args.batch, shuffle=False, collate_fn=collate)

    # 2. è·å–è¯è¡¨å¤§å°
    api_sz, status_sz, fine_names, _ = vocab_sizes_from_meta(args.data_root)
    # å¦‚æœ vocab.json é‡Œæ²¡æœ‰ fine_namesï¼Œè¯·æ‰‹åŠ¨æŒ‡å®šæˆ–ä» dataset ç»Ÿè®¡
    class_names = fine_names if fine_names else [f"Type_{i}" for i in range(10)]
    num_classes = len(class_names)
    print(f"ğŸ¯ Classes ({num_classes}): {class_names}")

    # 3. åˆå§‹åŒ–æ¨¡å‹ (å¸¦ Context)
    model = TraceClassifier(
        api_vocab=api_sz, 
        status_vocab=status_sz, 
        num_classes=num_classes,
        ctx_dim=args.ctx_dim # å…³é”®ï¼šä¼ å…¥ Context ç»´åº¦
    ).to(device)
    
    opt = torch.optim.AdamW(model.parameters(), lr=args.lr)
    
    # Loss å®šä¹‰
    ce_loss = nn.CrossEntropyLoss()
    bce_loss = nn.BCEWithLogitsLoss()

    # 4. è®­ç»ƒå¾ªç¯
    best_f1 = 0.0
    
    for ep in range(1, args.epochs+1):
        model.train()
        total_loss = 0
        
        pbar = tqdm(tr_loader, desc=f"Epoch {ep}/{args.epochs}")
        for g, y, _, _ in pbar:
            g = g.to(device); y = y.to(device)
            
            # Forward
            out = model(g)
            
            # è®¡ç®— Loss (å¤šä»»åŠ¡ï¼šåˆ†ç±» + äºŒåˆ†ç±»è¾…åŠ©)
            # å‡è®¾ 0 å·ç±»æ˜¯ Normal
            is_anomaly = (y > 0).float()
            
            if isinstance(out, dict):
                # æ¨èæ–¹å¼ï¼šå¤šå¤´ Loss
                loss_type = ce_loss(out["logits_type"], y)
                loss_bin = bce_loss(out["logit_bin"], is_anomaly)
                loss = loss_type + 0.5 * loss_bin # æƒé‡å¯è°ƒ
            else:
                # å…¼å®¹æ—§ä»£ç 
                loss = ce_loss(out, y)
                
            opt.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)
            opt.step()
            
            total_loss += loss.item()
            pbar.set_postfix(loss=loss.item())
            
        # Validation
        print(f"\n[Eval Epoch {ep}]")
        # ç›´æ¥ä½¿ç”¨ utils_sv ä¸­çš„å‡½æ•°ï¼Œå®ƒç°åœ¨èƒ½å¤„ç† dict äº†
        metrics = evaluate_detailed(model, va_loader, device, class_names)
        
        # metrics æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œæ ¹æ® utils_sv çš„è¿”å›å€¼è·å– acc/f1
        acc = metrics["acc"]
        f1 = metrics["macro_f1"]
        print(f"Val Acc: {acc:.4f} | Val F1: {f1:.4f}")
        
        if f1 > best_f1:
            best_f1 = f1
            torch.save(model.state_dict(), os.path.join(args.save_dir, "best_model.pth"))
            print("âœ¨ New Best Model Saved!")

    # 5. Final Test
    print("\nğŸ† Final Test Evaluation")
    model.load_state_dict(torch.load(os.path.join(args.save_dir, "best_model.pth")))
    evaluate_detailed(model, te_loader, device, class_names)
    
    # ä¿å­˜ stats ç”¨äºæ¨ç†
    import pickle
    with open(os.path.join(args.save_dir, "stats.pkl"), "wb") as f:
        pickle.dump(stats, f)
    print("âœ… Training Complete.")

if __name__ == "__main__":
    main()